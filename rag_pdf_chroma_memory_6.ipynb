{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask questions about your own collection of pdfs.\n",
    "# The pdfs are loaded into a chroma database.\n",
    "## Items we see here\n",
    "### reading in all pdfs in a specified directory\n",
    "### splitting up the pdfs into chunks and vectorizing them\n",
    "### storing vectors to a chroma database\n",
    "### load all vectors from chroma database\n",
    "### for specified questions get similar documents\n",
    "### langchain -> ask questions using the loaded documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.)  Takes a list of questions to ask about one input directory of pdfs.\n",
    "2.)  Similarity search on questions.\n",
    "3.)  YES:  memory in this example.\n",
    "\n",
    "For RAG Remembering chat history use \n",
    "https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db\n",
    "\n",
    "...use ConversationalRetrievalChain\n",
    "https://medium.com/@kbdhunga/enhancing-conversational-ai-the-power-of-langchains-question-answer-framework-4974e1cab3cf\n",
    "section on Memory Based Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "#load_dotenv()\n",
    "dotenv_path = Path('.env_secure')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "open_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "dotenv_path = Path('.env_rag_0006')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "text_chunk_size = int(os.environ.get('TEXT_CHUNK_SIZE'))\n",
    "text_chunk_overlap = int(os.environ.get('TEXT_CHUNK_OVERLAP'))\n",
    "query_file_name = os.environ.get('QUERY_FILE_NAME') + '.txt'\n",
    "input_pdf_directory = os.environ.get('INPUT_PDF_DIRECTORY')\n",
    "input_query_directory = os.environ.get('INPUT_QUERY_DIRECTORY')\n",
    "chroma_directory = './' + os.environ.get('CHROMA_DIRECTORY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test info\n",
      "WARNING:root:test warning\n",
      "ERROR:root:test error\n",
      "CRITICAL:root:test critical\n"
     ]
    }
   ],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "logging.debug('test debug')\n",
    "logging.info('test info')\n",
    "logging.warning('test warning')\n",
    "logging.error('test error')\n",
    "logging.critical('test critical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to simplify things just delete the contents of the chroma directory and start fresh\n",
    "#if 'in use' error, restart kernel and try again\n",
    "import os, shutil\n",
    "folder = chroma_directory\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.docstore.document import Document\n",
    "#from langchain.document_loaders import TextLoader\n",
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "#from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_query(specified_file_name):\n",
    "    specified_full_name = './' + input_query_directory + '/' + specified_file_name\n",
    "    logging.info(specified_full_name)\n",
    "    text_file = open(specified_full_name, \"r\")\n",
    "    data = text_file.read()\n",
    "    all_lines = data.splitlines()\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        logging.info(f'Exception: {Exception}')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_f(file_to_be_checked):\n",
    "    full_file_to_be_checked = './' + input_query_directory + '/' + file_to_be_checked\n",
    "    check_file = os.path.isfile(full_file_to_be_checked)\n",
    "    print(check_file)\n",
    "    if (not(check_file)):\n",
    "        this_err = f'error on file {full_file_to_be_checked}'\n",
    "        raise StopExecution(this_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:specific pdf list->['./datafiles2_input_pdf_0006\\\\afdb23-01_aeo_main_english_0602.pdf', './datafiles2_input_pdf_0006\\\\Enhancing_understanding_of_SARS-CoV-2_infection_am.pdf', './datafiles2_input_pdf_0006\\\\GEP-January-2023.pdf', './datafiles2_input_pdf_0006\\\\Prevalence_of_tuberculosis_COVID-19_chronic_condit.pdf', './datafiles2_input_pdf_0006\\\\Ramosetal.2023.Hypsolebiaslulai.pdf', './datafiles2_input_pdf_0006\\\\rep-0523.pdf', './datafiles2_input_pdf_0006\\\\Siapatisetal2023.pdf', './datafiles2_input_pdf_0006\\\\SocioeconomicStatusoftheFishingHouseholdsInsightfromSomeSelectedCoastalAreaofBangladesh.pdf', './datafiles2_input_pdf_0006\\\\Variations_in_growth_performance_of_Catla_Cattla_c.pdf']\n",
      "INFO:root:number pdf documents->9\n"
     ]
    }
   ],
   "source": [
    "#get all pdf files in directory\n",
    "#step 1.)  fill up loaders\n",
    "\n",
    "import glob\n",
    "from os import path\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "a_list = []\n",
    "full_path = './' + input_pdf_directory + '/*.pdf'\n",
    "a_list = glob.glob(full_path)\n",
    "pdf_loaders = []\n",
    "logging.info(f'specific pdf list->{a_list}')\n",
    "for one_item in a_list:\n",
    "    a_loader = PyPDFLoader(one_item)\n",
    "    pdf_loaders.append(a_loader)\n",
    "logging.info(f'number pdf documents->{len(pdf_loaders)}')\n",
    "\n",
    "pdf_page_chunks = []\n",
    "for loader in pdf_loaders:\n",
    "    pdf_page_chunks.extend(loader.load())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:number of data chunks->2809\n"
     ]
    }
   ],
   "source": [
    "# Define the Text Splitter \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = text_chunk_size,\n",
    "    chunk_overlap = text_chunk_overlap\n",
    ")\n",
    "\n",
    "#Create a split of the document using the text splitter\n",
    "splits = text_splitter.split_documents(pdf_page_chunks)\n",
    "logging.info(f'number of data chunks->{len(splits)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:root:vector database number of stored entries->2809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = chroma_directory\n",
    "# Create the vector store\n",
    "ids = [str(i) for i in range(1, len(splits) + 1)]\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "logging.info(f'vector database number of stored entries->{vectordb._collection.count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/76551067/how-to-create-a-langchain-doc-from-an-str\n",
    "#basically you get back a list of tuples instead of a list of document types\n",
    "#list massages the data to take a tuple and return it as a langchain Document type\n",
    "#so this is only necessary when with score is specified\n",
    "#remove duplicate scores just in case database is not deleted prior to run and there\n",
    "#are duplicate entries\n",
    "class score_docs:\n",
    "    def __init__(self, score_similarity):\n",
    "        self.score_similarity = score_similarity\n",
    "        self.score_list = []\n",
    "\n",
    "    def get_each_docs(self, one_with_score_item):\n",
    "        local_document = Document(page_content=one_with_score_item[0].page_content, metadata=one_with_score_item[0].metadata)\n",
    "        local_document.metadata.update({\"score\":one_with_score_item[1]})\n",
    "        return local_document\n",
    "    \n",
    "    def process_list(self):\n",
    "        parsed_docs = []\n",
    "        logging.info(len(self.score_similarity))\n",
    "        for one_returned_item in self.score_similarity:\n",
    "            the_returned_doc = self.get_each_docs(one_returned_item)\n",
    "            if the_returned_doc.metadata[\"score\"] in self.score_list:\n",
    "                logging.info(f'duplicate similarity found, skipping -> {the_returned_doc.metadata[\"score\"]}')\n",
    "            else:\n",
    "                self.score_list.append(the_returned_doc.metadata[\"score\"])\n",
    "                parsed_docs.append(the_returned_doc)\n",
    "        return parsed_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:./datafiles2_input_query_0006/specified_query.txt\n",
      "INFO:root:note type is list of tuples0><class 'list'> / <class 'tuple'>\n",
      "INFO:root:5\n"
     ]
    }
   ],
   "source": [
    "#get list of questions\n",
    "all_questions = get_query(query_file_name)\n",
    "q_len = len(all_questions)\n",
    "if(q_len < 4):\n",
    "    this_err = f'query question length wrong, len->{q_len}'\n",
    "    raise StopExecution(this_err)\n",
    "\n",
    "question = all_questions[0]\n",
    "search_results = vectordb.similarity_search_with_score(question,k=5)\n",
    "docs = search_results\n",
    "logging.info(f'note type is list of tuples0>{type(docs)} / {type(docs[0])}')\n",
    "\n",
    "similar_items = score_docs(search_results)\n",
    "similar_items_list = similar_items.process_list()\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2809\n"
     ]
    }
   ],
   "source": [
    "#reference-> https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed\n",
    "#load vector db \n",
    "# Load vector database that was persisted earlier and check collection count in it\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = chroma_directory\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='of Central African States tightened monetary policy \\nin 2022 by raising key rates several times to respond \\nto inflationary pressures and boost foreign reserves. \\nInflation rose to 4.2% in 2022 from 1.1% in 2021 due \\nto higher food prices and the effects of Russia’s inva -\\nsion of Ukraine. The fiscal balance turned to a surplus \\nof 0.8% in 2022 from a deficit of 1.1% in 2021 due to \\nhigher oil revenue (up 51.8%). Debt fell to 52.6% of GDP \\nin 2022 from 66.0% in 2021 thanks to lower financing', metadata={'page': 180, 'source': './datafiles2_input_pdf_0006\\\\afdb23-01_aeo_main_english_0602.pdf', 'score': 0.2246154546737671}), Document(page_content='Africa’s average consumer price inflation \\nis projected to increase from an estimated \\n14.2\\xa0percent in 2022 to 15.1\\xa0percent in 2023, \\nand to decline to 9.5\\xa0 percent in 2024. The \\nprojected increase in 2023 mirrors structural \\nweaknesses in most African countries: supply \\nconstraints to offset the effects of elevated food \\nprices, dependence on energy imports, even in \\nkey oil producers such as Nigeria, and exchange \\nrate passthrough effects from the stronger US', metadata={'page': 15, 'source': './datafiles2_input_pdf_0006\\\\afdb23-01_aeo_main_english_0602.pdf', 'score': 0.23779988288879395}), Document(page_content='the growing season.\\nIn light of the strong appreciation of the dollar, \\nexchange rate dynamics in Africa were mixed, \\nwith the majority of the continent’s currencies \\ndepreciating against the US dollar in 2022 (figure \\n1.11). Beyond the spike in inflationary pressures, \\nthe US Federal Reserve’s aggressive interest rate \\nhikes in 2022 heightened global uncertainty and \\ncontributed to investors exiting assets of emerging \\nmarket economies, including African currencies, \\ntoward safe US treasuries.', metadata={'page': 38, 'source': './datafiles2_input_pdf_0006\\\\afdb23-01_aeo_main_english_0602.pdf', 'score': 0.23999302089214325}), Document(page_content='30 AFRICA ’S ECONOMIC  PERFORMANCE  AND  OUTLOOKInflation in Africa \\nis projected to \\nincrease further to a \\nrecord 15.1\\xa0percent \\nin 2023 but decline \\nto 9.5\\xa0percent in \\n2024, close to the \\npre-pandemic levels \\nof 9\\xa0percent in 2019 \\nand 9.7\\xa0percent \\nin 2014–18inflation in Africa is due to domestic factors such \\nas drought, expansionary public investment, and, \\nmore important, the direct effect of imported \\ninflation, and external factors such as rising oil \\nand food prices, exacerbated by supply chain', metadata={'page': 41, 'source': './datafiles2_input_pdf_0006\\\\afdb23-01_aeo_main_english_0602.pdf', 'score': 0.2408011257648468}), Document(page_content='eral countries.\\nThe consumer price inflation in Africa rose by \\nan estimated 1.3\\xa0percentage points to 14.2\\xa0per -\\ncent in 2022 from 12.9\\xa0 percent in 2021  — a \\n0.5\\xa0percentage point revision from the estimated \\n13.8\\xa0percent in Africa’s Macroeconomic Outlook  \\n2023 published in January. The strong rise of \\nFIGURE 1.12  Global Supply Chain Pressures Index\\nStandard deviations from average value\\nMar.\\n20232022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010–1012345', metadata={'page': 40, 'source': './datafiles2_input_pdf_0006\\\\afdb23-01_aeo_main_english_0602.pdf', 'score': 0.24089212715625763})]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "question = all_questions[0]\n",
    "docs = vectordb.similarity_search(question,k=5)\n",
    "len(docs)\n",
    "docs = vectordb.similarity_search_with_score(question,k=5)\n",
    "similar_items = score_docs(docs)\n",
    "similar_items_list = similar_items.process_list()\n",
    "\n",
    "print(similar_items_list)\n",
    "print(len(similar_items_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_question(particular_question, search_ty):\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "    llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "    retriever = vectordb.as_retriever(search_type=search_ty, search_kwargs={'k': 3, 'fetch_k': 10, 'lambda_mult': .8})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    query = particular_question\n",
    "    result = qa_chain({\"query\": query})\n",
    "    print(f'search type:{search_ty}/result->{result[\"result\"]}')\n",
    "    base_name = os.path.basename(result['source_documents'][0].metadata['source'])\n",
    "    base_name_page = base_name + '(page:' + str(result['source_documents'][0].metadata['page']) + ')'\n",
    "    print(f\"specific metadata->{base_name_page}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What fueled strong inflation pressures in African economies in 2022?\n",
      "search type:mmr/result->The strong inflation pressures in African economies in 2022 were fueled by higher food prices and the effects of Russia's invasion of Ukraine.\n",
      "specific metadata->afdb23-01_aeo_main_english_0602.pdf(page:180)\n",
      "search type:similarity/result->The strong inflation pressures in African economies in 2022 were fueled by higher food prices and the effects of Russia's invasion of Ukraine.\n",
      "specific metadata->afdb23-01_aeo_main_english_0602.pdf(page:180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:looping->1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Down syndrome (DS) characterized by?\n",
      "search type:mmr/result->Down syndrome (DS) is characterized by intellectual and developmental disabilities, facial dysmorphisms, muscular hypotonia, and numerous birth defects, including cardiac and gastrointestinal anomalies. Individuals with DS also have immune defects, making them more susceptible to autoimmune diseases and infections.\n",
      "specific metadata->Enhancing_understanding_of_SARS-CoV-2_infection_am.pdf(page:0)\n",
      "search type:similarity/result->Down syndrome (DS) is characterized by intellectual and developmental disabilities, facial dysmorphisms, muscular hypotonia, and numerous birth defects, including cardiac and gastrointestinal anomalies. Individuals with DS also have immune defects, making them more susceptible to autoimmune diseases and infections.\n",
      "specific metadata->Enhancing_understanding_of_SARS-CoV-2_infection_am.pdf(page:0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:looping->2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the World Bank Evolution Roadmap?\n",
      "search type:mmr/result->The World Bank Evolution Roadmap is a document being developed by the World Bank to address the challenges faced by developing countries due to the COVID-19 pandemic and other factors such as the Ukraine war. It aims to propose a way forward for the World Bank in supporting countries' efforts to achieve sustainable, inclusive, and resilient development. The roadmap also includes an enhanced formulation of the World Bank's mission, which is to end extreme poverty and boost shared prosperity.\n",
      "specific metadata->GEP-January-2023.pdf(page:3)\n",
      "search type:similarity/result->The World Bank Evolution Roadmap is a document being developed by the World Bank at a critical moment in time. It takes into account the impact of the COVID-19 pandemic and the Ukraine war on developing countries, which has led to loss of fiscal buffers, increased indebtedness, and erosion of creditworthiness. The roadmap aims to propose a way forward for the World Bank based on discussions in the Board and suggests an enhanced formulation of the World Bank mission to end extreme poverty and boost shared prosperity through sustainable, inclusive, and resilient development.\n",
      "specific metadata->GEP-January-2023.pdf(page:3)\n"
     ]
    }
   ],
   "source": [
    "loop_max = 2\n",
    "loop_cnt = 0\n",
    "import time\n",
    "for this_question in all_questions:\n",
    "    print(this_question)\n",
    "    one_question(this_question, 'mmr')\n",
    "    time.sleep(5)\n",
    "    one_question(this_question, 'similarity')\n",
    "    time.sleep(5)\n",
    "    if loop_cnt >= loop_max:\n",
    "        break\n",
    "    loop_cnt += 1\n",
    "    logging.info(f'looping->{loop_cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have the vector database loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is a simple demonstration of chat history with RAG.  \n",
    "This chain has two steps. First, it condenses the current question and the chat history into a standalone question. This is necessary to create a standanlone vector to use for retrieval. After that, it does retrieval and then answers the question using retrieval augmented generation with a separate model. Part of the power of the declarative nature of LangChain is that you can easily use a separate language model for each call. This can be useful to use a cheaper and faster model for the simpler task of condensing the question, and then a more expensive model for answering the question. Here is an example of doing so.\n",
    "(https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Where was the first reported record of the Atlantic soft pout?', 'chat_history': [], 'answer': 'The first reported record of the Atlantic soft pout, Melanostigma atlanticum, was in the eastern Mediterranean Sea.'}\n",
      "result->The first reported record of the Atlantic soft pout, Melanostigma atlanticum, was in the eastern Mediterranean Sea.\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "{'question': 'How was a specimen caught?', 'chat_history': [('Where was the first reported record of the Atlantic soft pout?', 'The first reported record of the Atlantic soft pout, Melanostigma atlanticum, was in the eastern Mediterranean Sea.')], 'answer': 'The specimen of the Atlantic soft pout was caught in a pelagic trawl during a research trip carried out north of the island of Crete (Greece) in December 2019.'}\n",
      "result->The specimen of the Atlantic soft pout was caught in a pelagic trawl during a research trip carried out north of the island of Crete (Greece) in December 2019.\n",
      "<class 'list'>\n",
      "[('Where was the first reported record of the Atlantic soft pout?', 'The first reported record of the Atlantic soft pout, Melanostigma atlanticum, was in the eastern Mediterranean Sea.'), ('How was a specimen caught?', 'The specimen of the Atlantic soft pout was caught in a pelagic trawl during a research trip carried out north of the island of Crete (Greece) in December 2019.')]\n",
      "{'question': 'What waters does it inhabit?', 'chat_history': [('Where was the first reported record of the Atlantic soft pout?', 'The first reported record of the Atlantic soft pout, Melanostigma atlanticum, was in the eastern Mediterranean Sea.'), ('How was a specimen caught?', 'The specimen of the Atlantic soft pout was caught in a pelagic trawl during a research trip carried out north of the island of Crete (Greece) in December 2019.')], 'answer': 'The Atlantic soft pout (Melanostigma atlanticum) inhabits waters between 400 and 1800 meters deep on the continental slope.'}\n",
      "result->The Atlantic soft pout (Melanostigma atlanticum) inhabits waters between 400 and 1800 meters deep on the continental slope.\n",
      "<class 'list'>\n",
      "[('Where was the first reported record of the Atlantic soft pout?', 'The first reported record of the Atlantic soft pout, Melanostigma atlanticum, was in the eastern Mediterranean Sea.'), ('How was a specimen caught?', 'The specimen of the Atlantic soft pout was caught in a pelagic trawl during a research trip carried out north of the island of Crete (Greece) in December 2019.'), ('What waters does it inhabit?', 'The Atlantic soft pout (Melanostigma atlanticum) inhabits waters between 400 and 1800 meters deep on the continental slope.')]\n"
     ]
    }
   ],
   "source": [
    "#uses article Simpatisetal2023.pdf\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "retriever = vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 3, 'fetch_k': 10, 'lambda_mult': .8})\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'),\n",
    ")\n",
    "\n",
    "particular_question = 'Where was the first reported record of the Atlantic soft pout?'\n",
    "query = particular_question\n",
    "chat_history = []\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result)\n",
    "\n",
    "print(f'result->{result[\"answer\"]}')\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "print(type(chat_history))\n",
    "print(type(chat_history[0]))\n",
    "\n",
    "particular_question = 'How was a specimen caught?'\n",
    "query = particular_question\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result)\n",
    "\n",
    "print(f'result->{result[\"answer\"]}')\n",
    "chat_history.append(tuple((query, result[\"answer\"])))\n",
    "print(type(chat_history))\n",
    "print(chat_history)\n",
    "\n",
    "particular_question = 'What waters does it inhabit?'\n",
    "query = particular_question\n",
    "result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result)\n",
    "\n",
    "print(f'result->{result[\"answer\"]}')\n",
    "chat_history.append(tuple((query, result[\"answer\"])))\n",
    "print(type(chat_history))\n",
    "print(chat_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_z26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
