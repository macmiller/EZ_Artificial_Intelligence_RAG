{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Same as last exercise but with DB.  We will use Chroma as a introductory learning DB, evenually will use postgresql (with Vector extension) for production.\n",
    "\n",
    "We introduce here:\n",
    ">> langchain retriever\n",
    "\n",
    "Still not doing similarity search here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "load_dotenv()\n",
    "dotenv_path = Path('.env_secure')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "open_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "dotenv_path = Path('.env_rag_0005')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "max_filename_length = int(str(os.environ.get('MAX_FILENAME_LENGTH')))\n",
    "url_strip_num_characters = int(os.environ.get('URL_STRIP_NUM_CHARACTERS'))\n",
    "file_indicator_size = int(os.environ.get('FILE_INDICATOR_SIZE'))\n",
    "text_chunk_size = int(os.environ.get('TEXT_CHUNK_SIZE'))\n",
    "text_chunk_overlap = int(os.environ.get('TEXT_CHUNK_OVERLAP'))\n",
    "output_directory = os.environ.get('OUTPUT_DIRECTORY')\n",
    "file_selection = os.environ.get(\"FILE_SELECTION\")\n",
    "url_file_name = os.environ.get('URL_FILE_NAME') + '_' + file_selection + '.txt'\n",
    "query_file_name = os.environ.get('QUERY_FILE_NAME') + '_' + file_selection + '.txt'\n",
    "input_directory = os.environ.get('INPUT_DIRECTORY')\n",
    "max_documents = int(os.environ.get('MAX_DOCUMENTS'))\n",
    "chroma_directory = os.environ.get('CHROMA_DIRECTORY')\n",
    "print(max_documents)\n",
    "#logging_is_active_str = os.environ.get('LOGGING_IS_ACTIVE')\n",
    "#logging_is_active = False\n",
    "#if logging_is_active_str == 'True':\n",
    "#    logging_is_active = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "logging.debug('test debug')\n",
    "logging.info('test info')\n",
    "logging.warning('test warning')\n",
    "logging.error('test error')\n",
    "logging.critical('test critical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms import OpenAI \n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from tqdm import tqdm\n",
    "import tqdm\n",
    "\n",
    "someCounter = 0\n",
    "maxCounter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_contents(specified_file_name):\n",
    "    file_full_name = './' + input_directory + '/' + specified_file_name\n",
    "    text_file = open(file_full_name, \"r\")\n",
    "    data = text_file.read()\n",
    "    all_lines = data.splitlines()\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_files():\n",
    "    cleared_count = 0\n",
    "    dir = './' + output_directory\n",
    "    for f in os.listdir(dir):\n",
    "        os.remove(os.path.join(dir,f))\n",
    "        cleared_count += 1\n",
    "    return cleared_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_files(base_text, base_name):\n",
    "    global someCounter\n",
    "    global maxCounter\n",
    "    logging.info(f'')\n",
    "    string_split = str(someCounter).zfill(file_indicator_size)\n",
    "    filename = base_name + string_split\n",
    "    try:\n",
    "        f = open(f\"./{output_directory}/{filename}.txt\", \"x\")\n",
    "        f.write(base_text)\n",
    "        f.close()\n",
    "        maxCounter += 1\n",
    "    except:\n",
    "        print(f'file exists {filename}, skipping')\n",
    "        logging.info(f'file exists {filename}, skipping')\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "\n",
    "def check_f(file_to_be_checked):\n",
    "    full_file_to_be_checked = './' + input_directory + '/' + file_to_be_checked\n",
    "    check_file = os.path.isfile(full_file_to_be_checked)\n",
    "    print(check_file)\n",
    "    if (not(check_file)):\n",
    "        print(f'error on file {full_file_to_be_checked}')\n",
    "        raise StopExecution\n",
    "\n",
    "check_f(url_file_name)\n",
    "check_f(query_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gets text from dynamic or static web page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "def retrieve_text_from_url(inbound_url):\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(inbound_url)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html)\n",
    "    print(soup)\n",
    "    all_text = ''\n",
    "    for tag in soup.find_all('p'):\n",
    "        all_text += tag.text + '\\n'\n",
    "    return(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = clear_files()\n",
    "print(f\"{ret} old files cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []\n",
    "\n",
    "def check_f(file_to_be_checked):\n",
    "    full_file_to_be_checked = './' + input_directory + '/' + file_to_be_checked\n",
    "    check_file = os.path.isfile(full_file_to_be_checked)\n",
    "    print(check_file)\n",
    "    if (not(check_file)):\n",
    "        print(f'error on file {full_file_to_be_checked}')\n",
    "        raise StopExecution\n",
    "\n",
    "check_f(url_file_name)\n",
    "check_f(query_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = get_file_contents(url_file_name)\n",
    "print(url_list)\n",
    "print(query_file_name)\n",
    "query_list = get_file_contents(query_file_name)\n",
    "print(query_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_alternate_names = []\n",
    "for aitem in url_list:\n",
    "    aitem_short = aitem[url_strip_num_characters:]\n",
    "    aitem_short = aitem_short.replace(\"/\", \"_\")\n",
    "    aitem_short = aitem_short[:max_filename_length]\n",
    "    url_alternate_names.append(aitem_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have a text chunk then we need to save it to a file\n",
    "#In this example what we do is save each text chunk to a text file\n",
    "#we will later then load the text chunks\n",
    "import time\n",
    "\n",
    "for someUrl in enumerate(url_list):\n",
    "    global someCounter\n",
    "    someCounter = 0 \n",
    "    url_name = someUrl[1]\n",
    "    url_index = someUrl[0]\n",
    "    all_text_from_url = retrieve_text_from_url(url_name)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=text_chunk_size, chunk_overlap=text_chunk_overlap)\n",
    "    someSplit = text_splitter.split_text(all_text_from_url)\n",
    "    for one_text in someSplit:\n",
    "        if(maxCounter >= max_documents):\n",
    "            break\n",
    "        ret_val = flush_files(one_text, url_alternate_names[url_index])\n",
    "        someCounter += 1\n",
    "    time.sleep(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "#def get_text_chunks_langchain(text):\n",
    "#   text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "#   docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
    "#   return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we start the questioning, first without the LLM knowing what we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for specific_query in query_list:\n",
    "    print(specific_query)\n",
    "\n",
    "    print(llm(specific_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "file_path = './' + output_directory\n",
    "print(file_path)\n",
    "# Load multiple files\n",
    "loaders = [TextLoader(os.path.join(file_path, fn)) for fn in os.listdir(file_path)]\n",
    "print(loaders)\n",
    "\n",
    "all_documents = []\n",
    "for loader in loaders:\n",
    "    print(\"Loading raw document\" + loader.file_path)\n",
    "    raw_documents = loader.load()\n",
    "    all_documents.extend(raw_documents)\n",
    "\n",
    "print(all_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will save the documents to chromadb\n",
    "chroma_directory_full = './' + chroma_directory\n",
    "db = Chroma.from_documents(documents=all_documents, embedding=embedding_function, persist_directory=chroma_directory_full)\n",
    "db.persist()\n",
    "db = None\n",
    "print(chroma_directory_full)\n",
    "db = Chroma(persist_directory=chroma_directory_full,\n",
    "            embedding_function=embedding_function)\n",
    "retriever = db.as_retriever()\n",
    "print(all_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://smngeo.medium.com/vector-search-with-large-language-models-ee1aaa4bcccf\n",
    "#good easy example vectordb.similarity_search(query, k=5) \n",
    "\n",
    "#https://www.reddit.com/r/LangChain/comments/12qdgq7/whats_the_difference_between/\n",
    "\n",
    "#https://www.timescale.com/blog/how-to-build-llm-applications-with-pgvector-vector-store-in-langchain/\n",
    "#good example on similarity search and all usage of PGVector\n",
    "\n",
    "This is from https://community.openai.com/t/do-retrieval-augmented-generation-systems-answer-out-of-context-questions/317460\n",
    "\n",
    "I recommend running this script with and without this as context.\n",
    "\n",
    "A question is never out-of-context or out of domain-answerable knowledge, unless you specifically instruct the AI that it is. The GPT-3+ models already comes with the ability to answer most anything you would feed it already.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/samwit/langchain-tutorials/blob/main/YT_HF_Instructor_Embeddings_Chroma_DB_Multi_Doc_Retriever_LangChain_Part2.ipynb\n",
    "#https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "for specific_query in query_list:\n",
    "    print(specific_query)\n",
    "    #also note can use chain.run\n",
    "    query_response = chain(specific_query)\n",
    "    print(query_response['query'])\n",
    "    print('>>' + query_response['result'])\n",
    "    #print(query_response.query)\n",
    "    #print(query_response.result)\n",
    "\n",
    "#response = chain(query)\n",
    "#print(response)\n",
    "\n",
    "#query = \"Please summarize RAG and what it is useful for\"\n",
    "#response = chain(query)\n",
    "#print(response)\n",
    "\n",
    "#query = \"Please explain some important concepts in RAG and why it is so important to the field of Artifical Intellegence\"\n",
    "#response = chain.run(query)\n",
    "#print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same exercise but using ChromaDB\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_z26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
